{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image-Based Product Description Generator\n",
        "\n",
        "This Python script generates **marketing-oriented product descriptions** based on visual input. It utilizes a **Vision-to-Text model** called `SmolVLM-Instruct`, provided by Hugging Face. The model takes an image and, based on a specific textual prompt, produces a relevant marketing description for the image.\n",
        "\n",
        "The script is optimized for **Google Colab** and designed to run efficiently with **TPU v4**, allowing fast processing of large datasets and rapid generation of image-based descriptions.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Features**\n",
        "\n",
        "* **Model**: `SmolVLM-Instruct`, a vision-language model capable of interpreting both images and text.\n",
        "* **Objective**: Generate compelling, e-commerce-friendly marketing descriptions for each image.\n",
        "* **Outputs**:\n",
        "\n",
        "  * A `.txt` file containing detailed product descriptions.\n",
        "  * A `.csv` file listing image names alongside their generated descriptions.\n",
        "* **Google Colab & TPU v4**: Leveraging TPU significantly enhances speed and scalability when working with multiple images.\n",
        "\n",
        "---\n",
        "\n",
        "# **How to Run the Script in Google Colab**\n",
        "\n",
        "### 1. **Open Google Colab**\n",
        "\n",
        "Visit [Google Colab](https://colab.research.google.com) and start a new Python 3 notebook.\n",
        "\n",
        "### 2. **Enable TPU v4**\n",
        "\n",
        "To enable TPU:\n",
        "\n",
        "* Click **Runtime** ‚Üí **Change runtime type**\n",
        "* Set **Hardware accelerator** to **TPU**\n",
        "\n",
        "### 3. **Install Required Libraries**\n",
        "\n",
        "Run the following commands in a code cell to install dependencies:\n",
        "\n",
        "```python\n",
        "!pip install torch torchvision\n",
        "!pip install transformers\n",
        "!pip install Pillow\n",
        "```\n",
        "\n",
        "### 4. **Upload and Run the Script**\n",
        "\n",
        "Upload your Python script into the Colab environment. Paste the code into a cell and run it. The script will:\n",
        "\n",
        "* Load images from the specified folder,\n",
        "* Generate a description for each,\n",
        "* Save the results into `.txt` and `.csv` files.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yn35mx4DFmxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code Overview ‚Äî In-Depth Explanation**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image\n",
        "```\n",
        "\n",
        "### üîç Explanation:\n",
        "\n",
        "* **`torch`**: The PyTorch library is used for tensor computation and to manage model operations, including device placement (CPU/GPU/TPU).\n",
        "* **`csv`**: Built-in Python module to create and write structured `.csv` files ‚Äî essential for exporting data in tabular format.\n",
        "* **`os`**: Provides functions for interacting with the operating system ‚Äî used here to read directory contents and manipulate file paths.\n",
        "* **`re`**: Regular expressions are used for pattern-based text manipulation ‚Äî in this case, to clean up and parse model outputs.\n",
        "* **`PIL (Image)`**: From the `Pillow` library, allows loading and processing of image files.\n",
        "* **`transformers`**: Hugging Face‚Äôs library for working with pre-trained language and vision models:\n",
        "\n",
        "  * **`AutoProcessor`**: Automatically loads the appropriate pre-processing pipeline (image and text).\n",
        "  * **`AutoModelForVision2Seq`**: Loads a model that can take an image and output text (Vision-to-Text).\n",
        "  * **`load_image`**: A utility to properly load and convert image files into a format the model can accept.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "L9wOQOHoIAC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image"
      ],
      "metadata": {
        "id": "fX9EsHb5-JQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Device Selection**\n",
        "\n",
        "```python\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* This line dynamically assigns the model to the most suitable processing hardware:\n",
        "\n",
        "  * **\"cuda\"** for NVIDIA GPUs,\n",
        "  * **\"cpu\"** if no GPU is available.\n",
        "* This ensures optimal performance without hardcoding the device.\n",
        "* If you‚Äôre running the notebook on **TPU** (as in Google Colab), additional setup is required, but this fallback handles most local and cloud setups.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Load Processor & Model**\n",
        "\n",
        "```python\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_flash_attention_2=False\n",
        ")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* Loads the pre-trained model **`SmolVLM-Instruct`**, which is designed for image-to-text tasks.\n",
        "* The **processor** prepares the input data by:\n",
        "\n",
        "  * Formatting text and image inputs,\n",
        "  * Applying tokenization and image transformation,\n",
        "  * Creating input tensors for the model.\n",
        "* Model parameters:\n",
        "\n",
        "  * **`torch_dtype=torch.bfloat16`**: Uses efficient 16-bit precision (bfloat16) for memory optimization on supported hardware.\n",
        "  * **`device_map=\"auto\"`**: Automatically maps model layers to available devices (e.g., Colab TPUs or GPUs).\n",
        "  * **`use_flash_attention_2=False`**: Disables Flash Attention, which can offer speed benefits on specific setups, but may be incompatible in Colab TPU contexts.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Dev7EtX0INZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_flash_attention_2=False\n",
        ")"
      ],
      "metadata": {
        "id": "SW-EDkNY-QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Load Image Files**\n",
        "\n",
        "```python\n",
        "folder_path = \"/content\"\n",
        "allowed_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
        "image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(allowed_extensions)]\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* **`folder_path`**: Sets the directory where your images are located (e.g., Colab `/content`).\n",
        "* **`allowed_extensions`**: Filters acceptable image formats to avoid incompatible or unsupported file types.\n",
        "* **`image_paths`**: Constructs a list of full file paths to images that match the allowed extensions.\n",
        "\n",
        "  * This list will later be iterated over to generate descriptions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OTmCanMQIXDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content\"\n",
        "allowed_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
        "image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(allowed_extensions)]"
      ],
      "metadata": {
        "id": "UFEQPyUk-Wjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Prompt for Description Generation**\n",
        "\n",
        "```python\n",
        "prompt_text = \"Based on this product image, write a persuasive, marketing-focused description for an e-commerce website. Highlight the product‚Äôs features, benefits, and the value it offers to the user.\"\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* This is the **instruction given to the model**. It's crafted to align with e-commerce goals:\n",
        "\n",
        "  * **Persuasion**: Encourages the model to write with marketing language.\n",
        "  * **Structure**: Prompts inclusion of features, benefits, and user value.\n",
        "* Customizing this prompt allows different types of descriptions, e.g., technical specs, social media captions, or SEO-focused summaries.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Prepare List for Outputs**\n",
        "\n",
        "```python\n",
        "descriptions = []\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* Initializes an empty list to store the results.\n",
        "* Each entry will be a **tuple**: `(image_filename, generated_description)`\n",
        "* This list is later used to write both `.txt` and `.csv` files.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2lBiHOKEIolF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"Based on this product image, write a persuasive, marketing-focused description for an e-commerce website. Highlight the product‚Äôs features, benefits, and the value it offers to the user.\"\n",
        "\n",
        "descriptions = []"
      ],
      "metadata": {
        "id": "L7TtDdjiIrlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **7‚Äì11. Generate Descriptions from Images**\n",
        "\n",
        "```python\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    try:\n",
        "        image = load_image(image_path)\n",
        "        image_name = os.path.basename(image_path)\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "        raw_output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        generated_text = re.split(r\"Assistant:\\s*\", raw_output, maxsplit=1)[-1].strip()\n",
        "\n",
        "        descriptions.append((image_name, generated_text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* **Image Loading**: Uses `load_image` to convert the file into a format readable by the model.\n",
        "* **Message Template**: Follows a conversational format (like a chat with an AI assistant).\n",
        "* **Prompt Generation**: The `apply_chat_template` function wraps your message into a structure the model expects (especially for instruction-tuned models).\n",
        "* **Input Preparation**: Inputs are tokenized, converted to tensors, and sent to the appropriate device.\n",
        "* **Text Generation**:\n",
        "\n",
        "  * `generate()` produces the model‚Äôs response.\n",
        "  * `max_new_tokens=300` limits the length of the generated description.\n",
        "  * `repetition_penalty=1.2` reduces repeated words, improving diversity and readability.\n",
        "* **Output Cleaning**:\n",
        "\n",
        "  * Removes any unwanted prefixes (e.g., ‚ÄúAssistant:‚Äù) from the output.\n",
        "  * Uses regex and `.strip()` to clean whitespace.\n",
        "* **Appending to List**:\n",
        "\n",
        "  * Each image‚Äôs name and its corresponding generated description are saved for later export.\n",
        "* **Error Handling**:\n",
        "\n",
        "  * If an image is corrupted or the model fails to process it, the error is caught and printed, allowing the loop to continue.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wuJCPSqTI3bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    try:\n",
        "        image = load_image(image_path)\n",
        "        image_name = os.path.basename(image_path)\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "\n",
        "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "\n",
        "        raw_output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        generated_text = re.split(r\"Assistant:\\s*\", raw_output, maxsplit=1)[-1].strip()\n",
        "\n",
        "        descriptions.append((image_name, generated_text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "-c2TZPDQ-hQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12. Save to Text File**\n",
        "\n",
        "```python\n",
        "with open(\"product_descriptions.txt\", \"w\") as f:\n",
        "    for image_name, description in descriptions:\n",
        "        f.write(f\"Image: {image_name}\\nDescription: {description}\\n\\n\")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* Exports the descriptions in **plain text** format for easy readability or review.\n",
        "* Each entry includes:\n",
        "\n",
        "  * The image filename.\n",
        "  * The generated product description.\n",
        "* Useful for manual QA or copying into documents/emails.\n",
        "\n",
        "---\n",
        "\n",
        "## **13. Save to CSV File**\n",
        "\n",
        "```python\n",
        "with open(\"product_descriptions.csv\", mode=\"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Image\", \"Description\"])\n",
        "    writer.writerows(descriptions)\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* Exports the results in a **CSV format** for structured data use.\n",
        "* Each row contains:\n",
        "\n",
        "  * Column 1: Image file name.\n",
        "  * Column 2: Generated description.\n",
        "* Ideal for importing into:\n",
        "\n",
        "  * E-commerce CMS platforms,\n",
        "  * Excel/Google Sheets,\n",
        "  * Databases or data pipelines.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nNWu1obOI8f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"product_descriptions.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
        "    for i, (img_name, desc) in enumerate(descriptions, 1):\n",
        "        txt_file.write(f\"--- Product {i} ({img_name}) ---\\n{desc}\\n\\n\")\n",
        "\n",
        "\n",
        "with open(\"product_descriptions.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"Image Name\", \"Description\"])\n",
        "    writer.writerows(descriptions)\n",
        "\n",
        "print(\"\\n Descriptions saved to 'product_descriptions.txt' and 'product_descriptions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYbZNn0z_AoC",
        "outputId": "d6a266d0-8325-41e1-91cb-edbcaa01f06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Descriptions saved to 'product_descriptions.txt' and 'product_descriptions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "By running this Python script, you will automatically generate and save marketing-focused product descriptions for all supported image files.\n",
        "\n",
        "## Output Files:\n",
        "\n",
        "* **`product_descriptions.txt`**\n",
        "  Contains each image‚Äôs name and its generated description in plain text format.\n",
        "\n",
        "* **`product_descriptions.csv`**\n",
        "  Structured data file with image names and their corresponding descriptions. Ideal for import into databases or e-commerce backends.\n",
        "\n",
        "---\n",
        "\n",
        "# **Use Cases**\n",
        "\n",
        "* **E-Commerce Platforms**: Use these descriptions as product copy for your listings.\n",
        "* **Marketing Teams**: Generate visual-based text for ads, banners, or campaigns.\n",
        "* **Data Science / ML**: Use the descriptions for supervised learning or text analysis.\n",
        "\n",
        "---\n",
        "\n",
        "# **Important Notes**\n",
        "\n",
        "* This script runs **efficiently on Google Colab with TPU v4**, which is highly recommended for large image batches.\n",
        "* Only images with `.jpg`, `.jpeg`, or `.png` extensions in the specified folder will be processed.\n",
        "* Any corrupted or unsupported image will be skipped and logged in the output.\n"
      ],
      "metadata": {
        "id": "3Ih2G_ZmGGYu"
      }
    }
  ]
}